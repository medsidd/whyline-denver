name: Realtime BigQuery Load

on:
  schedule:
    # Offset by ~2 minutes from the snapshot workflow to allow files to land
    - cron: "2-59/5 * * * *"
  workflow_dispatch:

jobs:
  load:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure gcloud credentials
        env:
          GCP_SERVICE_ACCOUNT: ${{ secrets.GOOGLE_APPLICATION_CREDENTIALS }}
        run: |
          echo "$GCP_SERVICE_ACCOUNT" > ${{ github.workspace }}/gcp-key.json
          echo "GOOGLE_APPLICATION_CREDENTIALS=${{ github.workspace }}/gcp-key.json" >> $GITHUB_ENV

      - name: Load GTFS-RT snapshots to BigQuery
        env:
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          BQ_DATASET_RAW: ${{ secrets.BQ_DATASET_RAW }}
        run: make bq-load-realtime

      - name: Run realtime marts
        env:
          DBT_PROFILES_DIR: ${{ github.workspace }}/dbt/profiles
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          BQ_DATASET_RAW: ${{ secrets.BQ_DATASET_RAW }}
          BQ_DATASET_STG: ${{ secrets.BQ_DATASET_STG }}
          BQ_DATASET_MART: ${{ secrets.BQ_DATASET_MART }}
          DBT_TARGET: prod
        run: make dbt-run-realtime
